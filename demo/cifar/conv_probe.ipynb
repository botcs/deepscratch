{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR images...\n",
      "Constructing network...\n",
      "Network ID: 140664322121616\n",
      "Network layout:\n",
      "------------------------------\n",
      "\tINPUT  (3, 32, 32)\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |1|\n",
      "  convolution  (10, 28, 28)   ->   kernels: (3, 10, 5, 5)\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |2|\n",
      "  max pool  (10, 14, 14)   ->   pool shape: (2, 2)\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |3|\n",
      "  activation (10, 14, 14)   ->   type: relu\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |4|\n",
      "  convolution  (10, 10, 10)   ->   kernels: (10, 10, 5, 5)\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |5|\n",
      "  activation (10, 10, 10)   ->   type: relu\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |6|\n",
      "  shaper  (1000,)\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |7|\n",
      "  fully connected  (500,)   ->   weights + bias: (500, 1000) + (500,)\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |8|\n",
      "  activation (500,)   ->   type: tanh\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |9|\n",
      "  fully connected  (300,)   ->   weights + bias: (300, 500) + (300,)\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |10|\n",
      "  activation (300,)   ->   type: tanh\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |11|\n",
      "  fully connected  (10,)   ->   weights + bias: (10, 300) + (10,)\n",
      "\t   |\n",
      "\t   |\n",
      "\t  |12|\n",
      "  \tOUTPUT  (10,)   ->   CRITERION  (softmax)\n",
      "------------------------------\n",
      "Working with network: newtest1-2conv-3abstract-layer500-300-10\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import network_module as nm\n",
    "import argparse\n",
    "\n",
    "def print_csv(filename, data):\n",
    "    with open(filename, 'wb') as out:\n",
    "        for t in data:\n",
    "            out.write('{}\\t{}\\n'.format(*t))\n",
    "\n",
    "\n",
    "# layer_params = sys.argv[1:]\n",
    "\n",
    "# netname = 'mnist-fc-784-'\n",
    "# for width in layer_params:\n",
    "#     netname += '{}-'.format(width)\n",
    "# netname += '10'\n",
    "reg = 0\n",
    "L1 = False\n",
    "L2 = False\n",
    "L05 = False\n",
    "netname = 'newtest1-2conv-3abstract-layer500-300-10'\n",
    "data_source = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "def loadcifar():\n",
    "    import cPickle\n",
    "    import os.path\n",
    "\n",
    "    if not os.path.exists('./cifar-10-batches-py/data_batch_1'):\n",
    "        print 'Downloading train data from', data_source\n",
    "        import urllib\n",
    "        import tarfile\n",
    "        if not os.path.exists('./cifar-10-batches-py/'):\n",
    "            os.makedirs('./cifar-10-batches-py/')\n",
    "            \n",
    "        dest = \"./cifar-10-batches-py/data.tar.gz\"\n",
    "        urllib.urlretrieve(data_source, dest)\n",
    "        print 'unzipping train data'\n",
    "        tar = tarfile.open(dest, \"r:gz\")\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    \n",
    "    \n",
    "    # Load the dataset\n",
    "    f = open('./cifar-10-batches-py/data_batch_1', 'rb')\n",
    "    train = cPickle.load(f)\n",
    "    f.close()\n",
    "    train_data = train['data'].reshape(-1,3,32,32).astype(float)\n",
    "    train_data /= 255.\n",
    "    label = np.array(train['labels'])\n",
    "    train_onehot = np.zeros((label.size, label.max() + 1))\n",
    "    train_onehot[np.arange(label.size), label] = 1\n",
    "\n",
    "    f = open('./cifar-10-batches-py/test_batch', 'rb')\n",
    "    test = cPickle.load(f)\n",
    "    f.close()\n",
    "    test_data = test['data'][:1000].reshape(-1,3,32,32).astype(float)\n",
    "    test_data /= 255.\n",
    "    label = np.array(test['labels'])\n",
    "    test_onehot = np.zeros((label.size, label.max() + 1))\n",
    "    test_onehot[np.arange(label.size), label] = 1\n",
    "\n",
    "\n",
    "    return [(train_data, train_onehot), (test_data, test_onehot)]\n",
    "\n",
    "\n",
    "print 'Loading CIFAR images...'\n",
    "train, test = loadcifar()\n",
    "train = (train[0][:1000], train[1][:1000])\n",
    "test = (test[0][:100], test[1][:100])\n",
    "\n",
    "print 'Constructing network...'\n",
    "#########################\n",
    "# NETWORK DEFINITION\n",
    "nn = nm.network(in_shape=train[0][0].shape, criterion='softmax')\n",
    "nn.add_conv(10, (5,5))\n",
    "nn.add_maxpool()\n",
    "nn.add_activation('relu')\n",
    "nn.add_conv(10, (5,5))\n",
    "nn.add_activation('relu')\n",
    "nn.add_shaper(np.prod(nn[-1].shape))\n",
    "nn.add_full(500)\n",
    "nn.add_activation('tanh')\n",
    "nn.add_full(300)\n",
    "nn.add_activation('tanh')\n",
    "nn.add_full(10)\n",
    "#########################\n",
    "print nn\n",
    "\n",
    "result = []\n",
    "\n",
    "\n",
    "def print_test():\n",
    "    print ' --- Epoch: ', nn.last_epoch, ' error: ',\\\n",
    "          nn.output.get_crit(train[0][0:16], train[1][0:16]).mean()\n",
    "\n",
    "print 'Working with network:', netname\n",
    "def train_net():\n",
    "  print 'Training network:', netname\n",
    "  nn.SGD(train_policy=nn.fix_epoch,\n",
    "         training_set=train,\n",
    "         L2=True, reg=0.001,\n",
    "         batch=32, rate=0.3, epoch_call_back=print_test, epoch=5)\n",
    "\n",
    "  print 'Saving network snapshot to {}.net'.format(netname)\n",
    "  nn.save_state('./nets/' + netname + '.net')\n",
    "\n",
    "\n",
    "\n",
    "def loadnetwork(network_source):\n",
    "    global nn\n",
    "    nn = nm.load(network_source)\n",
    "\n",
    "    \n",
    "def imshow(im, cmap='Greys_r', interpol='None'):\n",
    "\n",
    "    if len(im.shape) == 2:\n",
    "        plt.imshow(im.squeeze(), cmap=cmap, interpolation=interpol)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    if len(im.shape) == 3:\n",
    "        for i, x in enumerate(im, 1):\n",
    "            plt.subplot(1, len(im), i)\n",
    "            plt.imshow(x.squeeze(), cmap=cmap, interpolation=interpol)\n",
    "            plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "            plt.axis('off')\n",
    "    if len(im.shape) == 4:\n",
    "        for irow, xrow in enumerate(im, 0):\n",
    "            for icol, x in enumerate(xrow, 1):\n",
    "                #print '\\r  ', len(im), len(xrow), irow * len(xrow) + icol\n",
    "                plt.subplot(len(im), len(xrow), irow * len(xrow) + icol)\n",
    "                plt.imshow(x.squeeze(), cmap=cmap, interpolation=interpol)\n",
    "                plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "                plt.axis('off')\n",
    "                \n",
    "    if len(im.shape) == 5:\n",
    "        for irow, xrow in enumerate(im, 0):\n",
    "            for icol, x in enumerate(xrow, 1):\n",
    "                #print '\\r  ', len(im), len(xrow), irow * len(xrow) + icol\n",
    "                plt.subplot(len(im), len(xrow), irow * len(xrow) + icol)\n",
    "                plt.imshow(x.squeeze(), cmap=cmap, interpolation=interpol)\n",
    "                plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "                plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    return im.shape\n",
    "\n",
    "\n",
    "def visualise_layer(lay_ind=4, top=9, iterations=1000):\n",
    "    test = nn.grad_ascent(lay_ind, train[0], top, iterations)\\\n",
    "             .reshape((top,) + nn[lay_ind].shape + (3, 32, 32))\n",
    "    test = np.concatenate((test, test.mean(axis=0)[np.newaxis, :]), axis=0)\n",
    "    return test\n",
    "\n",
    "\n",
    "def max_act(lay_ind, top=9):\n",
    "    return test[0][nn.max_act(lay_ind, test[0], top)].squeeze()\n",
    "    \n",
    "def gradient_check(eps=0.001):\n",
    "    nn[1].kernels[0,0,0,0] += eps\n",
    "    dp = nn.output.get_crit(test[0][0], test[1][0])[0]\n",
    "    nn[1].kernels[0,0,0,0] -= 2 * eps\n",
    "    dm = nn.output.get_crit(test[0][0], test[1][0])[0]    \n",
    "    nn[1].kernels[0,0,0,0] += eps\n",
    "    return (dp-dm)/2/eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_outputs():\n",
    "    nn.get_output(train[0][0:3])\n",
    "    imshow(nn[0].output)\n",
    "    imshow(nn[1].output)\n",
    "    imshow(nn[4].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network: newtest1-2conv-3abstract-layer500-300-10\n",
      "   batch: 31 of 31  --- Epoch:  1  error:  39.3749899744\n",
      "   batch: 31 of 31  --- Epoch:  2  error:  39.9972428269\n",
      "   batch: 31 of 31  --- Epoch:  3  error:  46.461240355\n",
      "   batch: 31 of 31  --- Epoch:  4  error:  37.0762411494\n",
      "   batch: 31 of 31  --- Epoch:  5  error:  38.1005890808\n",
      "Saving network snapshot to newtest1-2conv-3abstract-layer500-300-10.net\n"
     ]
    }
   ],
   "source": [
    "train_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.get_output(train[0][0])\n",
    "((nn[8].output[0,0]-nn[8].output[0,1])).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.test_eval(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3   0.34 ...,  0.17  0.21]\n",
      " [ 0.62  0.37 ...,  0.82  0.73]\n",
      " ..., \n",
      " [ 0.72  0.44 ...,  0.4   0.39]\n",
      " [ 0.61  0.64 ...,  0.68  0.14]]\n",
      "[[ 0.37  0.62 ...,  0.37  0.22]\n",
      " [ 0.57  0.56 ...,  0.6   0.19]\n",
      " ..., \n",
      " [ 0.94  0.8  ...,  0.81  0.55]\n",
      " [ 0.87  0.06 ...,  0.55  0.25]]\n",
      "[[ 0.93  0.57 ...,  0.22  0.3 ]\n",
      " [ 0.75  0.88 ...,  0.34  0.8 ]\n",
      " ..., \n",
      " [ 0.04  0.65 ...,  0.83  0.85]\n",
      " [ 0.69  0.24 ...,  0.71  0.3 ]]\n",
      "[[ 0.4   0.19 ...,  0.79  0.49]\n",
      " [ 0.17  0.51 ...,  0.18  0.39]\n",
      " ..., \n",
      " [ 0.97  0.48 ...,  0.7   0.27]\n",
      " [ 0.78  0.93 ...,  0.46  0.13]]\n",
      "[[ 0.98  0.24 ...,  1.    0.58]\n",
      " [ 0.2   0.9  ...,  0.03  0.23]\n",
      " ..., \n",
      " [ 0.74  0.26 ...,  0.29  0.62]\n",
      " [ 0.98  0.24 ...,  0.51  0.31]]\n"
     ]
    }
   ],
   "source": [
    "for k in nn[1].kernels[0]: print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
